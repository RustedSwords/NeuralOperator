{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e2a9e0a-6118-4629-8e25-0b91b4ae9ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from neuralop.models import TFNO\n",
    "from neuralop import Trainer\n",
    "from neuralop.training import CheckpointCallback\n",
    "from neuralop.datasets import load_darcy_flow_small\n",
    "from neuralop.utils import count_model_params\n",
    "from neuralop import LpLoss, H1Loss\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03c2ecfb-a26d-4dcb-8115-8217b55635c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test db at resolution 32 with 50 samples and batch-size=32\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loaders, data_processor = load_darcy_flow_small(\n",
    "        n_train=1000, batch_size=32,\n",
    "        test_resolutions=[16, 32], n_tests=[100, 50],\n",
    "        test_batch_sizes=[32, 32],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2484d824-441b-4c6a-92a5-10814b55ad54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our model has 523257 parameters.\n"
     ]
    }
   ],
   "source": [
    "model = TFNO(n_modes=(16, 16), hidden_channels=32, projection_channels=64, factorization='tucker', rank=0.42)\n",
    "model = model.to(device)\n",
    "\n",
    "n_params = count_model_params(model)\n",
    "print(f'\\nOur model has {n_params} parameters.')\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2668f21-f92f-4116-9228-51b421a863cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                lr=8e-3,\n",
    "                                weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59a88ccc-85d9-4739-b66b-00cfbf3250b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2loss = LpLoss(d=2, p=2)\n",
    "h1loss = H1Loss(d=2)\n",
    "\n",
    "train_loss = h1loss\n",
    "eval_losses={'h1': h1loss, 'l2': l2loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef9fe26f-c909-458f-97c6-c85ad7bdf952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### MODEL ###\n",
      " TFNO(\n",
      "  (fno_blocks): FNOBlocks(\n",
      "    (convs): SpectralConv(\n",
      "      (weight): ModuleList(\n",
      "        (0-3): 4 x ComplexTuckerTensor(shape=(32, 32, 16, 9), rank=(26, 26, 13, 7))\n",
      "      )\n",
      "    )\n",
      "    (fno_skips): ModuleList(\n",
      "      (0-3): 4 x Flattened1dConv(\n",
      "        (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lifting): MLP(\n",
      "    (fcs): ModuleList(\n",
      "      (0): Conv1d(3, 256, kernel_size=(1,), stride=(1,))\n",
      "      (1): Conv1d(256, 32, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      "  (projection): MLP(\n",
      "    (fcs): ModuleList(\n",
      "      (0): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
      "      (1): Conv1d(64, 1, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "### OPTIMIZER ###\n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.008\n",
      "    lr: 0.008\n",
      "    maximize: False\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "\n",
      "### SCHEDULER ###\n",
      " <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x75cdd5bf5a90>\n",
      "\n",
      "### LOSSES ###\n",
      "\n",
      " * Train: <neuralop.losses.data_losses.H1Loss object at 0x75cdc84b2d10>\n",
      "\n",
      " * Test: {'h1': <neuralop.losses.data_losses.H1Loss object at 0x75cdc84b2d10>, 'l2': <neuralop.losses.data_losses.LpLoss object at 0x75cdbeaaa1d0>}\n"
     ]
    }
   ],
   "source": [
    "print('\\n### MODEL ###\\n', model)\n",
    "print('\\n### OPTIMIZER ###\\n', optimizer)\n",
    "print('\\n### SCHEDULER ###\\n', scheduler)\n",
    "print('\\n### LOSSES ###')\n",
    "print(f'\\n * Train: {train_loss}')\n",
    "print(f'\\n * Test: {eval_losses}')\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00e28c78-79ca-4395-bb37-e8d4b14e05a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using standard method to load data to device.\n",
      "using standard method to compute loss.\n",
      "self.override_load_to_device=False\n",
      "self.overrides_loss=False\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model=model, n_epochs=20,\n",
    "                  device=device,\n",
    "                  callbacks=[\n",
    "                    CheckpointCallback(save_dir='./checkpoints',\n",
    "                                       save_interval=10,\n",
    "                                            save_optimizer=True,\n",
    "                                            save_scheduler=True)\n",
    "                        ],\n",
    "                  data_processor=data_processor,\n",
    "                  wandb_log=False,\n",
    "                  log_test_interval=3,\n",
    "                  use_distributed=False,\n",
    "                  verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "833c8e22-ba11-404b-bd3b-f6f586ffcb98",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain(train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[1;32m      2\u001b[0m               test_loaders\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m      3\u001b[0m               optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m      4\u001b[0m               scheduler\u001b[38;5;241m=\u001b[39mscheduler,\n\u001b[1;32m      5\u001b[0m               regularizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m               training_loss\u001b[38;5;241m=\u001b[39mtrain_loss)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# resume training from saved checkpoint at epoch 10\u001b[39;00m\n\u001b[1;32m     11\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model\u001b[38;5;241m=\u001b[39mmodel, n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m     12\u001b[0m                   device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     13\u001b[0m                   data_processor\u001b[38;5;241m=\u001b[39mdata_processor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m                   use_distributed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m                   verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Projects/Neural_operator/neuraloperator/neuralop/training/trainer.py:255\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, train_loader, test_loaders, optimizer, scheduler, regularizer, training_loss, eval_losses)\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mon_val_end()\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m--> 255\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mon_epoch_end(\n\u001b[1;32m    256\u001b[0m             epoch\u001b[38;5;241m=\u001b[39mepoch, train_err\u001b[38;5;241m=\u001b[39mtrain_err, avg_loss\u001b[38;5;241m=\u001b[39mavg_loss\n\u001b[1;32m    257\u001b[0m         )\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m errors\n",
      "File \u001b[0;32m~/Projects/Neural_operator/neuraloperator/neuralop/training/callbacks.py:197\u001b[0m, in \u001b[0;36mPipelineCallback.on_epoch_end\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_epoch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m--> 197\u001b[0m         c\u001b[38;5;241m.\u001b[39mon_epoch_end(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Projects/Neural_operator/neuraloperator/neuralop/training/callbacks.py:476\u001b[0m, in \u001b[0;36mCheckpointCallback.on_epoch_end\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    474\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 476\u001b[0m save_training_state(\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir,\n\u001b[1;32m    478\u001b[0m     model_name,\n\u001b[1;32m    479\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    480\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    481\u001b[0m     regularizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregularizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    482\u001b[0m     scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscheduler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    483\u001b[0m )\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved training state to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/Neural_operator/neuraloperator/neuralop/training/training_state.py:92\u001b[0m, in \u001b[0;36msave_training_state\u001b[0;34m(save_dir, save_name, model, optimizer, scheduler, regularizer)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m regularizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     regularizer_pth \u001b[38;5;241m=\u001b[39m save_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregularizer.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 92\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(regularizer\u001b[38;5;241m.\u001b[39mstate_dict(), regularizer_pth)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully saved training state to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'state_dict'"
     ]
    }
   ],
   "source": [
    "trainer.train(train_loader=train_loader,\n",
    "              test_loaders={},\n",
    "              optimizer=optimizer,\n",
    "              scheduler=scheduler,\n",
    "              regularizer=False,\n",
    "              training_loss=train_loss)\n",
    "\n",
    "\n",
    "# resume training from saved checkpoint at epoch 10\n",
    "\n",
    "trainer = Trainer(model=model, n_epochs=20,\n",
    "                  device=device,\n",
    "                  data_processor=data_processor,\n",
    "                  callbacks=[\n",
    "                    CheckpointCallback(save_dir='./new_checkpoints',\n",
    "                                            resume_from_dir='./checkpoints/ep_10')\n",
    "                        ],\n",
    "                  wandb_log=False,\n",
    "                  log_test_interval=3,\n",
    "                  use_distributed=False,\n",
    "                  verbose=True)\n",
    "\n",
    "trainer.train(train_loader=train_loader,\n",
    "              test_loaders={},\n",
    "              optimizer=optimizer,\n",
    "              scheduler=scheduler,\n",
    "              regularizer=False,\n",
    "              training_loss=train_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralop_env",
   "language": "python",
   "name": "neuralop_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
