{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mesh_datamodule.py',\n",
       " 'hdf5_dataset.py',\n",
       " '__pycache__',\n",
       " 'zarr_dataset.py',\n",
       " 'data',\n",
       " 'dict_dataset.py',\n",
       " 'burgers.py',\n",
       " 'navier_stokes.py',\n",
       " 'data_transforms.py',\n",
       " 'pt_dataset.py',\n",
       " 'tensor_dataset.py',\n",
       " 'output_encoder.py',\n",
       " 'tests',\n",
       " 'darcy.py',\n",
       " 'transforms.py',\n",
       " '__init__.py',\n",
       " 'spherical_swe.py']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_path=os.path.abspath('../../neuraloperator/neuralop/datasets')\n",
    "os.listdir(datasets_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0,datasets_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from neuralop.training.patching import MultigridPatching2D\n",
    "\n",
    "class Transform(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Applies transforms or inverse transforms to \n",
    "    model inputs or outputs, respectively\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    @abstractmethod\n",
    "    def transform(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def inverse_transform(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def cuda(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def cpu(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def to(self, device):\n",
    "        pass\n",
    "    \n",
    "class Normalizer():\n",
    "    def __init__(self, mean, std, eps=1e-6):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.eps = eps\n",
    "\n",
    "    def __call__(self, data):\n",
    "        return (data - self.mean)/(self.std + self.eps)\n",
    "\n",
    "class Composite(Transform):\n",
    "    def __init__(self, transforms: List[Transform]):\n",
    "        \"\"\"Composite transform composes a list of\n",
    "        Transforms into one Transform object.\n",
    "\n",
    "        Transformations are not assumed to be commutative\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        transforms : List[Transform]\n",
    "            list of transforms to be applied to data\n",
    "            in order\n",
    "        \"\"\"\n",
    "        super.__init__()\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def transform(self, data_dict):\n",
    "        for tform in self.transforms:\n",
    "            data_dict = tform.transform(self.data_dict)\n",
    "        return data_dict\n",
    "    \n",
    "    def inverse_transform(self, data_dict):\n",
    "        for tform in self.transforms[::-1]:\n",
    "            data_dict = tform.transform(self.data_dict)\n",
    "        return data_dict\n",
    "\n",
    "    def to(self, device):\n",
    "        # all Transforms are required to implement .to()\n",
    "        self.transforms = [t.to(device) for t in self.transforms if hasattr(t, 'to')]\n",
    "        return self\n",
    "\n",
    "class MGPatchingTransform(Transform):\n",
    "    def __init__(self, model: torch.nn.Module, levels: int, \n",
    "                 padding_fraction: float, stitching: float):\n",
    "        \"\"\"Wraps MultigridPatching2D to expose canonical\n",
    "        transform .transform() and .inverse_transform() API\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model: nn.Module\n",
    "            model to wrap in MultigridPatching2D\n",
    "        levels : int\n",
    "            mg_patching level parameter for MultigridPatching2D\n",
    "        padding_fraction : float\n",
    "            mg_padding_fraction parameter for MultigridPatching2D\n",
    "        stitching : float\n",
    "            mg_patching_stitching parameter for MultigridPatching2D\n",
    "        \"\"\"\n",
    "        super.__init__()\n",
    "\n",
    "        self.levels = levels\n",
    "        self.padding_fraction = padding_fraction\n",
    "        self.stitching = stitching\n",
    "        self.patcher = MultigridPatching2D(model=model, levels=self.levels, \n",
    "                                      padding_fraction=self.padding_fraction,\n",
    "                                      stitching=self.stitching)\n",
    "    def transform(self, data_dict):\n",
    "        \n",
    "        x = data_dict['x']\n",
    "        y = data_dict['y']\n",
    "\n",
    "        x,y = self.patcher.patch(x,y)\n",
    "\n",
    "        data_dict['x'] = x\n",
    "        data_dict['y'] = y\n",
    "        return data_dict\n",
    "    \n",
    "    def inverse_transform(self, data_dict):\n",
    "        x = data_dict['x']\n",
    "        y = data_dict['y']\n",
    "\n",
    "        x,y = self.patcher.unpatch(x,y)\n",
    "\n",
    "        data_dict['x'] = x\n",
    "        data_dict['y'] = y\n",
    "        return data_dict\n",
    "    \n",
    "    def to(self, _):\n",
    "        # nothing to pass to device\n",
    "        return self\n",
    "\n",
    "class RandomMGPatch():\n",
    "    def __init__(self, levels=2):\n",
    "        self.levels = levels\n",
    "        self.step = 2**levels\n",
    "\n",
    "    def __call__(self, data):\n",
    "\n",
    "        def _get_patches(shifted_image, step, height, width):\n",
    "            \"\"\"Take as input an image and return multi-grid patches centered around the middle of the image\n",
    "            \"\"\"\n",
    "            if step == 1:\n",
    "                return (shifted_image, )\n",
    "            else:\n",
    "                # Notice that we need to stat cropping at start_h = (height - patch_size)//2\n",
    "                # (//2 as we pad both sides)\n",
    "                # Here, the extracted patch-size is half the size so patch-size = height//2\n",
    "                # Hence the values height//4 and width // 4\n",
    "                start_h = height//4\n",
    "                start_w = width//4\n",
    "\n",
    "                patches = _get_patches(shifted_image[:, start_h:-start_h, start_w:-start_w], step//2, height//2, width//2)\n",
    "\n",
    "                return (shifted_image[:, ::step, ::step], *patches)\n",
    "        \n",
    "        x, y = data\n",
    "        channels, height, width = x.shape\n",
    "        center_h = height//2\n",
    "        center_w = width//2\n",
    "\n",
    "        # Sample a random patching position\n",
    "        pos_h = torch.randint(low=0, high=height, size=(1,))[0]\n",
    "        pos_w = torch.randint(low=0, high=width, size=(1,))[0]\n",
    "\n",
    "        shift_h = center_h - pos_h\n",
    "        shift_w = center_w - pos_w\n",
    "\n",
    "        shifted_x = torch.roll(x, (shift_h, shift_w), dims=(0, 1))\n",
    "        patches_x = _get_patches(shifted_x, self.step, height, width)\n",
    "        shifted_y = torch.roll(y, (shift_h, shift_w), dims=(0, 1))\n",
    "        patches_y = _get_patches(shifted_y, self.step, height, width)\n",
    "\n",
    "        return torch.cat(patches_x, dim=0), patches_y[-1]\n",
    "\n",
    "class MGPTensorDataset(Dataset):\n",
    "    def __init__(self, x, y, levels=2):\n",
    "        assert (x.size(0) == y.size(0)), \"Size mismatch between tensors\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.levels = 2\n",
    "        self.transform = RandomMGPatch(levels=levels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.transform((self.x[index], self.y[index]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.size(0)\n",
    "    \n",
    "\n",
    "def regular_grid(spatial_dims, grid_boundaries=[[0, 1], [0, 1]]):\n",
    "    \"\"\"\n",
    "    Appends grid positional encoding to an input tensor, concatenating as additional dimensions along the channels\n",
    "    \"\"\"\n",
    "    height, width = spatial_dims\n",
    "\n",
    "    xt = torch.linspace(grid_boundaries[0][0], grid_boundaries[0][1],\n",
    "                        height + 1)[:-1]\n",
    "    yt = torch.linspace(grid_boundaries[1][0], grid_boundaries[1][1],\n",
    "                        width + 1)[:-1]\n",
    "\n",
    "    grid_x, grid_y = torch.meshgrid(xt, yt, indexing='ij')\n",
    "\n",
    "    grid_x = grid_x.repeat(1, 1)\n",
    "    grid_y = grid_y.repeat(1, 1)\n",
    "\n",
    "    return grid_x, grid_y\n",
    "\n",
    "\n",
    "class PositionalEmbedding2D():\n",
    "    \"\"\"A simple positional embedding as a regular 2D grid\n",
    "    \"\"\"\n",
    "    def __init__(self, grid_boundaries=[[0, 1], [0, 1]]):\n",
    "        \"\"\"PositionalEmbedding2D applies a simple positional \n",
    "        embedding as a regular 2D grid\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        grid_boundaries : list, optional\n",
    "            coordinate boundaries of input grid, by default [[0, 1], [0, 1]]\n",
    "        \"\"\"\n",
    "        self.grid_boundaries = grid_boundaries\n",
    "        self._grid = None\n",
    "        self._res = None\n",
    "\n",
    "    def grid(self, spatial_dims, device, dtype):\n",
    "        \"\"\"grid generates 2D grid needed for pos encoding\n",
    "        and caches the grid associated with MRU resolution\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        spatial_dims : torch.size\n",
    "             sizes of spatial resolution\n",
    "        device : literal 'cpu' or 'cuda:*'\n",
    "            where to load data\n",
    "        dtype : str\n",
    "            dtype to encode data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            output grids to concatenate \n",
    "        \"\"\"\n",
    "        # handle case of multiple train resolutions\n",
    "        if self._grid is None or self._res != spatial_dims: \n",
    "            grid_x, grid_y = regular_grid(spatial_dims,\n",
    "                                      grid_boundaries=self.grid_boundaries)\n",
    "            grid_x = grid_x.to(device).to(dtype).unsqueeze(0).unsqueeze(0)\n",
    "            grid_y = grid_y.to(device).to(dtype).unsqueeze(0).unsqueeze(0)\n",
    "            self._grid = grid_x, grid_y\n",
    "            self._res = spatial_dims\n",
    "\n",
    "        return self._grid\n",
    "\n",
    "    def __call__(self, data, batched=True):\n",
    "        if not batched:\n",
    "            if data.ndim == 3:\n",
    "                data = data.unsqueeze(0)\n",
    "        batch_size = data.shape[0]\n",
    "        x, y = self.grid(data.shape[-2:], data.device, data.dtype)\n",
    "        out =  torch.cat((data, x.expand(batch_size, -1, -1, -1),\n",
    "                          y.expand(batch_size, -1, -1, -1)),\n",
    "                         dim=1)\n",
    "        # in the unbatched case, the dataloader will stack N \n",
    "        # examples with no batch dim to create one\n",
    "        if not batched and batch_size == 1: \n",
    "            return out.squeeze(0)\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import prod\n",
    "\n",
    "def count_model_params(model):\n",
    "    \"\"\"Returns the total number of parameters of a PyTorch model\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    One complex number is counted as two parameters (we count real and imaginary parts)'\n",
    "    \"\"\"\n",
    "    return sum(\n",
    "        [p.numel() * 2 if p.is_complex() else p.numel() for p in model.parameters()]\n",
    "    )\n",
    "\n",
    "\n",
    "def count_tensor_params(tensor, dims=None):\n",
    "    \"\"\"Returns the number of parameters (elements) in a single tensor, optionally, along certain dimensions only\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tensor : torch.tensor\n",
    "    dims : int list or None, default is None\n",
    "        if not None, the dimensions to consider when counting the number of parameters (elements)\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    One complex number is counted as two parameters (we count real and imaginary parts)'\n",
    "    \"\"\"\n",
    "    if dims is None:\n",
    "        dims = list(tensor.shape)\n",
    "    else:\n",
    "        dims = [tensor.shape[d] for d in dims]\n",
    "    n_params = prod(dims)\n",
    "    if tensor.is_complex():\n",
    "        return 2*n_params\n",
    "    return n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UnitGaussianNormalizer(Transform):\n",
    "    \"\"\"\n",
    "    UnitGaussianNormalizer normalizes data to be zero mean and unit std.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean=None, std=None, eps=1e-7, dim=None, mask=None):\n",
    "        \"\"\"\n",
    "        mean : torch.tensor or None\n",
    "            has to include batch-size as a dim of 1\n",
    "            e.g. for tensors of shape ``(batch_size, channels, height, width)``,\n",
    "            the mean over height and width should have shape ``(1, channels, 1, 1)``\n",
    "        std : torch.tensor or None\n",
    "        eps : float, default is 0\n",
    "            for safe division by the std\n",
    "        dim : int list, default is None\n",
    "            if not None, dimensions of the data to reduce over to compute the mean and std.\n",
    "\n",
    "            .. important::\n",
    "\n",
    "                Has to include the batch-size (typically 0).\n",
    "                For instance, to normalize data of shape ``(batch_size, channels, height, width)``\n",
    "                along batch-size, height and width, pass ``dim=[0, 2, 3]``\n",
    "\n",
    "        mask : torch.Tensor or None, default is None\n",
    "            If not None, a tensor with the same size as a sample,\n",
    "            with value 0 where the data should be ignored and 1 everywhere else\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        The resulting mean will have the same size as the input MINUS the specified dims.\n",
    "        If you do not specify any dims, the mean and std will both be scalars.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        UnitGaussianNormalizer instance\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer(\"mean\", mean)\n",
    "        self.register_buffer(\"std\", std)\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "\n",
    "        self.eps = eps\n",
    "        if mean is not None:\n",
    "            self.ndim = mean.ndim\n",
    "        if isinstance(dim, int):\n",
    "            dim = [dim]\n",
    "        self.dim = dim\n",
    "        self.n_elements = 0\n",
    "\n",
    "    def fit(self, data_batch):\n",
    "        self.update_mean_std(data_batch)\n",
    "\n",
    "    def partial_fit(self, data_batch, batch_size=1):\n",
    "        if 0 in list(data_batch.shape):\n",
    "            return\n",
    "        count = 0\n",
    "        n_samples = len(data_batch)\n",
    "        while count < n_samples:\n",
    "            samples = data_batch[count : count + batch_size]\n",
    "            # print(samples.shape)\n",
    "            # if batch_size == 1:\n",
    "            #     samples = samples.unsqueeze(0)\n",
    "            if self.n_elements:\n",
    "                self.incremental_update_mean_std(samples)\n",
    "            else:\n",
    "                self.update_mean_std(samples)\n",
    "            count += batch_size\n",
    "\n",
    "    def update_mean_std(self, data_batch):\n",
    "        self.ndim = data_batch.ndim  # Note this includes batch-size\n",
    "        if self.mask is None:\n",
    "            self.n_elements = count_tensor_params(data_batch, self.dim)\n",
    "            self.mean = torch.mean(data_batch, dim=self.dim, keepdim=True)\n",
    "            self.squared_mean = torch.mean(data_batch**2, dim=self.dim, keepdim=True)\n",
    "            self.std = torch.std(data_batch, dim=self.dim, keepdim=True)\n",
    "        else:\n",
    "            batch_size = data_batch.shape[0]\n",
    "            dim = [i - 1 for i in self.dim if i]\n",
    "            shape = [s for i, s in enumerate(self.mask.shape) if i not in dim]\n",
    "            self.n_elements = torch.count_nonzero(self.mask, dim=dim) * batch_size\n",
    "            self.mean = torch.zeros(shape)\n",
    "            self.std = torch.zeros(shape)\n",
    "            self.squared_mean = torch.zeros(shape)\n",
    "            data_batch[:, self.mask == 1] = 0\n",
    "            self.mean[self.mask == 1] = (\n",
    "                torch.sum(data_batch, dim=dim, keepdim=True) / self.n_elements\n",
    "            )\n",
    "            self.squared_mean = (\n",
    "                torch.sum(data_batch**2, dim=dim, keepdim=True) / self.n_elements\n",
    "            )\n",
    "            self.std = torch.std(data_batch, dim=self.dim, keepdim=True)\n",
    "\n",
    "    def incremental_update_mean_std(self, data_batch):\n",
    "        if self.mask is None:\n",
    "            n_elements = count_tensor_params(data_batch, self.dim)\n",
    "            dim = self.dim\n",
    "        else:\n",
    "            dim = [i - 1 for i in self.dim if i]\n",
    "            n_elements = torch.count_nonzero(self.mask, dim=dim) * data_batch.shape[0]\n",
    "            data_batch[:, self.mask == 1] = 0\n",
    "\n",
    "        self.mean = (1.0 / (self.n_elements + n_elements)) * (\n",
    "            self.n_elements * self.mean + torch.sum(data_batch, dim=dim, keepdim=True)\n",
    "        )\n",
    "        self.squared_mean = (1.0 / (self.n_elements + n_elements)) * (\n",
    "            self.n_elements * self.squared_mean\n",
    "            + torch.sum(data_batch**2, dim=dim, keepdim=True)\n",
    "        )\n",
    "        self.n_elements += n_elements\n",
    "\n",
    "        # 1/(n_i + n_j) * (n_i * sum(x_i^2)/n_i + sum(x_j^2) - (n_i*sum(x_i)/n_i + sum(x_j))^2)\n",
    "        # = 1/(n_i + n_j)  * (sum(x_i^2) + sum(x_j^2) - sum(x_i)^2 - 2sum(x_i)sum(x_j) - sum(x_j)^2))\n",
    "        # multiply by (n_i + n_j) / (n_i + n_j + 1) for unbiased estimator\n",
    "        self.std = torch.sqrt(self.squared_mean - self.mean**2) * self.n_elements / (self.n_elements - 1)\n",
    "\n",
    "    def transform(self, x):\n",
    "        return (x - self.mean) / (self.std + self.eps)\n",
    "\n",
    "    def inverse_transform(self, x):\n",
    "        return x * (self.std + self.eps) + self.mean\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.transform(x)\n",
    "\n",
    "    def cuda(self):\n",
    "        self.mean = self.mean.cuda()\n",
    "        self.std = self.std.cuda()\n",
    "        return self\n",
    "\n",
    "    def cpu(self):\n",
    "        self.mean = self.mean.cpu()\n",
    "        self.std = self.std.cpu()\n",
    "        return self\n",
    "\n",
    "    def to(self, device):\n",
    "        self.mean = self.mean.to(device)\n",
    "        self.std = self.std.to(device)\n",
    "        return self\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataset(cls, dataset, dim=None, keys=None, mask=None):\n",
    "        \"\"\"Return a dictionary of normalizer instances, fitted on the given dataset\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : pytorch dataset\n",
    "            each element must be a dict {key: sample}\n",
    "            e.g. {'x': input_samples, 'y': target_labels}\n",
    "        dim : int list, default is None\n",
    "            * If None, reduce over all dims (scalar mean and std)\n",
    "            * Otherwise, must include batch-dimensions and all over dims to reduce over\n",
    "        keys : str list or None\n",
    "            if not None, a normalizer is instanciated only for the given keys\n",
    "        \"\"\"\n",
    "        for i, data_dict in enumerate(dataset):\n",
    "            if not i:\n",
    "                if not keys:\n",
    "                    keys = data_dict.keys()\n",
    "        instances = {key: cls(dim=dim, mask=mask) for key in keys}\n",
    "        for i, data_dict in enumerate(dataset):\n",
    "            for key, sample in data_dict.items():\n",
    "                if key in keys:\n",
    "                    instances[key].partial_fit(sample.unsqueeze(0))\n",
    "        return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "\n",
    "class TensorDataset(Dataset):\n",
    "    def __init__(self, x, y, transform_x=None, transform_y=None):\n",
    "        assert (x.size(0) == y.size(0)), \"Size mismatch between tensors\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform_x = transform_x\n",
    "        self.transform_y = transform_y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        y = self.y[index]\n",
    "        \n",
    "        if self.transform_x is not None:\n",
    "            x = self.transform_x(x)\n",
    "\n",
    "        if self.transform_y is not None:\n",
    "            y = self.transform_y(y)\n",
    "\n",
    "        return {'x': x, 'y':y}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEmbedding2D():\n",
    "    \"\"\"A simple positional embedding as a regular 2D grid\n",
    "    \"\"\"\n",
    "    def __init__(self, grid_boundaries=[[0, 1], [0, 1]]):\n",
    "        \"\"\"PositionalEmbedding2D applies a simple positional \n",
    "        embedding as a regular 2D grid\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        grid_boundaries : list, optional\n",
    "            coordinate boundaries of input grid, by default [[0, 1], [0, 1]]\n",
    "        \"\"\"\n",
    "        self.grid_boundaries = grid_boundaries\n",
    "        self._grid = None\n",
    "        self._res = None\n",
    "\n",
    "    def grid(self, spatial_dims, device, dtype):\n",
    "        \"\"\"grid generates 2D grid needed for pos encoding\n",
    "        and caches the grid associated with MRU resolution\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        spatial_dims : torch.size\n",
    "             sizes of spatial resolution\n",
    "        device : literal 'cpu' or 'cuda:*'\n",
    "            where to load data\n",
    "        dtype : str\n",
    "            dtype to encode data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            output grids to concatenate \n",
    "        \"\"\"\n",
    "        # handle case of multiple train resolutions\n",
    "        if self._grid is None or self._res != spatial_dims: \n",
    "            grid_x, grid_y = regular_grid(spatial_dims,\n",
    "                                      grid_boundaries=self.grid_boundaries)\n",
    "            grid_x = grid_x.to(device).to(dtype).unsqueeze(0).unsqueeze(0)\n",
    "            grid_y = grid_y.to(device).to(dtype).unsqueeze(0).unsqueeze(0)\n",
    "            self._grid = grid_x, grid_y\n",
    "            self._res = spatial_dims\n",
    "\n",
    "        return self._grid\n",
    "\n",
    "    def __call__(self, data, batched=True):\n",
    "        if not batched:\n",
    "            if data.ndim == 3:\n",
    "                data = data.unsqueeze(0)\n",
    "        batch_size = data.shape[0]\n",
    "        x, y = self.grid(data.shape[-2:], data.device, data.dtype)\n",
    "        out =  torch.cat((data, x.expand(batch_size, -1, -1, -1),\n",
    "                          y.expand(batch_size, -1, -1, -1)),\n",
    "                         dim=1)\n",
    "        # in the unbatched case, the dataloader will stack N \n",
    "        # examples with no batch dim to create one\n",
    "        if not batched and batch_size == 1: \n",
    "            return out.squeeze(0)\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "import torch\n",
    "from neuralop.training.patching import MultigridPatching2D\n",
    "\n",
    "\n",
    "class DataProcessor(torch.nn.Module, metaclass=ABCMeta):\n",
    "    def __init__(self):\n",
    "        \"\"\"DataProcessor exposes functionality for pre-\n",
    "        and post-processing data during training or inference.\n",
    "\n",
    "        To be a valid DataProcessor within the Trainer requires\n",
    "        that the following methods are implemented:\n",
    "\n",
    "        - to(device): load necessary information to device, in keeping\n",
    "            with PyTorch convention\n",
    "        - preprocess(data): processes data from a new batch before being\n",
    "            put through a model's forward pass\n",
    "        - postprocess(out): processes the outputs of a model's forward pass\n",
    "            before loss and backward pass\n",
    "        - wrap(self, model):\n",
    "            wraps a model in preprocess and postprocess steps to create one forward pass\n",
    "        - forward(self, x):\n",
    "            forward pass providing that a model has been wrapped\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    @abstractmethod\n",
    "    def to(self, device):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def preprocess(self, x):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def postprocess(self, x):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def wrap(self, model):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "class DefaultDataProcessor(DataProcessor):\n",
    "    def __init__(\n",
    "        self, in_normalizer=None, out_normalizer=None, positional_encoding=None\n",
    "    ):\n",
    "        \"\"\"A simple processor to pre/post process data before training/inferencing a model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_normalizer : Transform, optional, default is None\n",
    "            normalizer (e.g. StandardScaler) for the input samples\n",
    "        out_normalizer : Transform, optional, default is None\n",
    "            normalizer (e.g. StandardScaler) for the target and predicted samples\n",
    "        positional_encoding : Processor, optional, default is None\n",
    "            class that appends a positional encoding to the input\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_normalizer = in_normalizer\n",
    "        self.out_normalizer = out_normalizer\n",
    "        self.positional_encoding = positional_encoding\n",
    "        self.device = \"cpu\"\n",
    "\n",
    "    def wrap(self, model):\n",
    "        self.model = model\n",
    "        return self\n",
    "\n",
    "    def to(self, device):\n",
    "        if self.in_normalizer is not None:\n",
    "            self.in_normalizer = self.in_normalizer.to(device)\n",
    "        if self.out_normalizer is not None:\n",
    "            self.out_normalizer = self.out_normalizer.to(device)\n",
    "        self.device = device\n",
    "        return self\n",
    "\n",
    "    def preprocess(self, data_dict, batched=True):\n",
    "        x = data_dict[\"x\"].to(self.device)\n",
    "        y = data_dict[\"y\"].to(self.device)\n",
    "\n",
    "        if self.in_normalizer is not None:\n",
    "            x = self.in_normalizer.transform(x)\n",
    "        if self.positional_encoding is not None:\n",
    "            x = self.positional_encoding(x, batched=batched)\n",
    "        if self.out_normalizer is not None and self.train:\n",
    "            y = self.out_normalizer.transform(y)\n",
    "\n",
    "        data_dict[\"x\"] = x\n",
    "        data_dict[\"y\"] = y\n",
    "\n",
    "        return data_dict\n",
    "\n",
    "    def postprocess(self, output, data_dict):\n",
    "        y = data_dict[\"y\"]\n",
    "        if self.out_normalizer and not self.train:\n",
    "            output = self.out_normalizer.inverse_transform(output)\n",
    "            y = self.out_normalizer.inverse_transform(y)\n",
    "        data_dict[\"y\"] = y\n",
    "        return output, data_dict\n",
    "\n",
    "    def forward(self, **data_dict):\n",
    "        data_dict = self.preprocess(data_dict)\n",
    "        output = self.model(data_dict[\"x\"])\n",
    "        output = self.postprocess(output)\n",
    "        return output, data_dict\n",
    "\n",
    "class IncrementalDataProcessor(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_normalizer=None, out_normalizer=None, \n",
    "                 positional_encoding=None, device = 'cpu',\n",
    "                 subsampling_rates=[2, 1], dataset_resolution=16, dataset_indices=[2,3], epoch_gap=10, verbose=False):\n",
    "        \"\"\"An incremental processor to pre/post process data before training/inferencing a model\n",
    "        In particular this processor first regularizes the input resolution based on the sub_list and dataset_indices\n",
    "        in the spatial domain based on a fixed number of epochs. We incrementally increase the resolution like done \n",
    "        in curriculum learning to train the model. This is useful for training models on large datasets with high\n",
    "        resolution data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_normalizer : Transform, optional, default is None\n",
    "            normalizer (e.g. StandardScaler) for the input samples\n",
    "        out_normalizer : Transform, optional, default is None\n",
    "            normalizer (e.g. StandardScaler) for the target and predicted samples\n",
    "        positional_encoding : Processor, optional, default is None\n",
    "            class that appends a positional encoding to the input\n",
    "        device : str, optional, default is 'cpu'\n",
    "            device 'cuda' or 'cpu' where computations are performed\n",
    "        subsampling_rates : list, optional, default is [2, 1]\n",
    "            list of subsampling rates to use\n",
    "        dataset_resolution : int, optional, default is 16\n",
    "            resolution of the input data\n",
    "        dataset_indices : list, optional, default is [2, 3]\n",
    "            list of indices of the dataset to slice to regularize the input resolution - Spatial Dimensions\n",
    "        epoch_gap : int, optional, default is 10\n",
    "            number of epochs to wait before increasing the resolution\n",
    "        verbose : bool, optional, default is False\n",
    "            if True, print the current resolution\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_normalizer = in_normalizer\n",
    "        self.out_normalizer = out_normalizer\n",
    "        self.positional_encoding = positional_encoding\n",
    "        self.device = device\n",
    "        self.sub_list = subsampling_rates\n",
    "        self.dataset_resolution = dataset_resolution\n",
    "        self.dataset_indices = dataset_indices\n",
    "        self.epoch_gap = epoch_gap\n",
    "        self.verbose = verbose\n",
    "        self.mode = \"Train\"\n",
    "        self.epoch = 0\n",
    "        \n",
    "        self.current_index = 0\n",
    "        self.current_logged_epoch = 0\n",
    "        self.current_sub = self.index_to_sub_from_table(self.current_index)\n",
    "        self.current_res = int(self.dataset_resolution / self.current_sub)   \n",
    "        \n",
    "        print(f'Original Incre Res: change index to {self.current_index}')\n",
    "        print(f'Original Incre Res: change sub to {self.current_sub}')\n",
    "        print(f'Original Incre Res: change res to {self.current_res}')\n",
    "            \n",
    "    def wrap(self, model):\n",
    "        self.model = model\n",
    "        return self\n",
    "\n",
    "    def to(self, device):\n",
    "        if self.in_normalizer is not None:\n",
    "            self.in_normalizer = self.in_normalizer.to(device)\n",
    "        if self.out_normalizer is not None:\n",
    "            self.out_normalizer = self.out_normalizer.to(device)\n",
    "        self.device = device\n",
    "        return self\n",
    "    \n",
    "    def epoch_wise_res_increase(self, epoch):\n",
    "        # Update the current_sub and current_res values based on the epoch\n",
    "        if epoch % self.epoch_gap == 0 and epoch != 0 and (\n",
    "                self.current_logged_epoch != epoch):\n",
    "            self.current_index += 1\n",
    "            self.current_sub = self.index_to_sub_from_table(self.current_index)\n",
    "            self.current_res = int(self.dataset_resolution / self.current_sub)\n",
    "            self.current_logged_epoch = epoch\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f'Incre Res Update: change index to {self.current_index}')\n",
    "                print(f'Incre Res Update: change sub to {self.current_sub}')\n",
    "                print(f'Incre Res Update: change res to {self.current_res}')\n",
    "\n",
    "    def index_to_sub_from_table(self, index):\n",
    "        # Get the sub value from the sub_list based on the index\n",
    "        if index >= len(self.sub_list):\n",
    "            return self.sub_list[-1]\n",
    "        else:\n",
    "            return self.sub_list[index]\n",
    "\n",
    "    def regularize_input_res(self, x, y):\n",
    "        # Regularize the input data based on the current_sub and dataset_name\n",
    "        for idx in self.dataset_indices:\n",
    "            indexes = torch.arange(0, x.size(idx), self.current_sub, device=self.device)\n",
    "            x = x.index_select(dim=idx, index=indexes)\n",
    "            y = y.index_select(dim=idx, index=indexes)\n",
    "        return x, y\n",
    "    \n",
    "    def step(self, loss=None, epoch=None, x=None, y=None):\n",
    "        if x is not None and y is not None:\n",
    "            self.epoch_wise_res_increase(epoch)\n",
    "            return self.regularize_input_res(x, y)\n",
    "        \n",
    "    def preprocess(self, data_dict, batched=True):\n",
    "        x = data_dict['x'].to(self.device)\n",
    "        y = data_dict['y'].to(self.device)\n",
    "\n",
    "        if self.in_normalizer is not None:\n",
    "            x = self.in_normalizer.transform(x)\n",
    "        if self.positional_encoding is not None:\n",
    "            x = self.positional_encoding(x, batched=batched)\n",
    "        if self.out_normalizer is not None and self.train:\n",
    "            y = self.out_normalizer.transform(y)\n",
    "        \n",
    "        if self.mode == \"Train\":\n",
    "            x, y = self.step(epoch=self.epoch, x=x, y=y)\n",
    "        \n",
    "        data_dict['x'] = x\n",
    "        data_dict['y'] = y\n",
    "\n",
    "        return data_dict \n",
    "\n",
    "    def postprocess(self, output, data_dict):\n",
    "        y = data_dict['y']\n",
    "        if self.out_normalizer and not self.train:\n",
    "            output = self.out_normalizer.inverse_transform(output)\n",
    "            y = self.out_normalizer.inverse_transform(y)\n",
    "        data_dict['y'] = y\n",
    "        return output, data_dict\n",
    "    \n",
    "    def forward(self, **data_dict):\n",
    "        data_dict = self.preprocess(data_dict)\n",
    "        output = self.model(data_dict['x'])\n",
    "        output = self.postprocess(output)\n",
    "        return output, data_dict\n",
    "    \n",
    "class MGPatchingDataProcessor(DataProcessor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        levels: int,\n",
    "        padding_fraction: float,\n",
    "        stitching: float,\n",
    "        device: str = \"cpu\",\n",
    "        in_normalizer=None,\n",
    "        out_normalizer=None,\n",
    "        positional_encoding=None,\n",
    "    ):\n",
    "        \"\"\"MGPatchingDataProcessor\n",
    "        Applies multigrid patching to inputs out-of-place\n",
    "        with an optional output encoder/other data transform\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model: nn.Module\n",
    "            model to wrap in MultigridPatching2D\n",
    "        levels : int\n",
    "            mg_patching level parameter for MultigridPatching2D\n",
    "        padding_fraction : float\n",
    "            mg_padding_fraction parameter for MultigridPatching2D\n",
    "        stitching : float\n",
    "            mg_patching_stitching parameter for MultigridPatching2D\n",
    "        in_normalizer : neuralop.datasets.transforms.Transform, optional\n",
    "            OutputEncoder to decode model inputs, by default None\n",
    "        in_normalizer : neuralop.datasets.transforms.Transform, optional\n",
    "            OutputEncoder to decode model outputs, by default None\n",
    "        positional_encoding : neuralop.datasets.transforms.PositionalEmbedding2D, optional\n",
    "            appends pos encoding to x if used\n",
    "        device : str, optional\n",
    "            device 'cuda' or 'cpu' where computations are performed\n",
    "        positional_encoding : neuralop.datasets.transforms.Transform, optional\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.levels = levels\n",
    "        self.padding_fraction = padding_fraction\n",
    "        self.stitching = stitching\n",
    "        self.patcher = MultigridPatching2D(\n",
    "            model=model,\n",
    "            levels=self.levels,\n",
    "            padding_fraction=self.padding_fraction,\n",
    "            stitching=self.stitching,\n",
    "        )\n",
    "        self.device = device\n",
    "\n",
    "        # set normalizers to none by default\n",
    "        self.in_normalizer, self.out_normalizer = None, None\n",
    "        if in_normalizer:\n",
    "            self.in_normalizer = in_normalizer.to(self.device)\n",
    "        if out_normalizer:\n",
    "            self.out_normalizer = out_normalizer.to(self.device)\n",
    "        self.positional_encoding = positional_encoding\n",
    "        self.model = None\n",
    "\n",
    "    def to(self, device):\n",
    "        self.device = device\n",
    "        if self.in_normalizer:\n",
    "            self.in_normalizer = self.in_normalizer.to(self.device)\n",
    "        if self.out_normalizer:\n",
    "            self.out_normalizer = self.out_normalizer.to(self.device)\n",
    "\n",
    "    def wrap(self, model):\n",
    "        self.model = model\n",
    "        return self\n",
    "\n",
    "    def preprocess(self, data_dict, batched=True):\n",
    "        \"\"\"\n",
    "        Preprocess data assuming that if encoder exists, it has\n",
    "        encoded all data during data loading\n",
    "\n",
    "        Params\n",
    "        ------\n",
    "\n",
    "        data_dict: dict\n",
    "            dictionary keyed with 'x', 'y' etc\n",
    "            represents one batch of data input to a model\n",
    "        batched: bool\n",
    "            whether the first dimension of 'x', 'y' represents batching\n",
    "        \"\"\"\n",
    "        data_dict = {\n",
    "            k: v.to(self.device) for k, v in data_dict.items() if torch.is_tensor(v)\n",
    "        }\n",
    "        x, y = data_dict[\"x\"], data_dict[\"y\"]\n",
    "        if self.in_normalizer:\n",
    "            x = self.in_normalizer.transform(x)\n",
    "            y = self.out_normalizer.transform(y)\n",
    "        if self.positional_encoding is not None:\n",
    "            x = self.positional_encoding(x, batched=batched)\n",
    "        data_dict[\"x\"], data_dict[\"y\"] = self.patcher.patch(x, y)\n",
    "        return data_dict\n",
    "\n",
    "    def postprocess(self, out, data_dict):\n",
    "        \"\"\"\n",
    "        Postprocess model outputs, including decoding\n",
    "        if an encoder exists.\n",
    "\n",
    "        Params\n",
    "        ------\n",
    "\n",
    "        data_dict: dict\n",
    "            dictionary keyed with 'x', 'y' etc\n",
    "            represents one batch of data input to a model\n",
    "        out: torch.Tensor\n",
    "            model output predictions\n",
    "        \"\"\"\n",
    "        y = data_dict[\"y\"]\n",
    "        out, y = self.patcher.unpatch(out, y)\n",
    "\n",
    "        if self.out_normalizer:\n",
    "            y = self.out_normalizer.inverse_transform(y)\n",
    "            out = self.out_normalizer.inverse_transform(out)\n",
    "\n",
    "        data_dict[\"y\"] = y\n",
    "\n",
    "        return out, data_dict\n",
    "\n",
    "    def forward(self, **data_dict):\n",
    "        data_dict = self.preprocess(data_dict)\n",
    "        output = self.model(**data_dict)\n",
    "        output, data_dict = self.postprocess(output, data_dict)\n",
    "        return output, data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/rusted/Projects/Neural_operator/neuraloperator/neuralop/datasets/data'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path=os.path.abspath('../../neuraloperator/neuralop/datasets/data')\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_resolution=16\n",
    "channel_dim=1\n",
    "n_train=1000\n",
    "batch_size=32\n",
    "test_resolutions=[16, 32]\n",
    "n_tests=[100, 50]\n",
    "test_batch_sizes=[32, 32]\n",
    "encode_input=False\n",
    "encode_output=True\n",
    "encoding=\"channel-wise\"\n",
    "positional_encoding=True\n",
    "grid_boundaries=[[0, 1], [0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load the Navier-Stokes dataset\"\"\"\n",
    "data = torch.load(\n",
    "    Path(data_path).joinpath(f\"darcy_train_{train_resolution}.pt\").as_posix()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([[[ True,  True,  True,  ...,  True,  True, False],\n",
       "          [ True,  True,  True,  ...,  True, False, False],\n",
       "          [ True,  True,  True,  ...,  True,  True, False],\n",
       "          ...,\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False]],\n",
       " \n",
       "         [[ True,  True,  True,  ...,  True,  True,  True],\n",
       "          [ True,  True,  True,  ...,  True,  True,  True],\n",
       "          [False,  True,  True,  ...,  True,  True,  True],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ..., False, False,  True],\n",
       "          [ True,  True,  True,  ...,  True,  True,  True],\n",
       "          [ True,  True, False,  ...,  True,  True,  True]],\n",
       " \n",
       "         [[ True,  True,  True,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[False, False,  True,  ..., False, False, False],\n",
       "          [False, False,  True,  ..., False, False, False],\n",
       "          [False, False,  True,  ..., False, False, False],\n",
       "          ...,\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False]],\n",
       " \n",
       "         [[ True,  True,  True,  ...,  True,  True,  True],\n",
       "          [ True,  True,  True,  ...,  True,  True,  True],\n",
       "          [ True,  True,  True,  ...,  True,  True,  True],\n",
       "          ...,\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ..., False,  True, False]],\n",
       " \n",
       "         [[ True,  True,  True,  ..., False, False, False],\n",
       "          [ True,  True, False,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False,  True],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ...,  True, False, False],\n",
       "          [ True,  True,  True,  ...,  True, False, False]]]),\n",
       " 'y': tensor([[[1.1645e-05, 3.8866e-04, 6.2636e-04,  ..., 2.0518e-03,\n",
       "           1.4713e-03, 3.0848e-03],\n",
       "          [3.8803e-04, 2.0817e-02, 3.5712e-02,  ..., 1.3034e-01,\n",
       "           1.2955e-01, 1.3645e-01],\n",
       "          [6.2186e-04, 3.5467e-02, 6.2444e-02,  ..., 2.4814e-01,\n",
       "           2.5193e-01, 2.0839e-01],\n",
       "          ...,\n",
       "          [8.5640e-03, 4.6371e-01, 7.5949e-01,  ..., 8.6734e-01,\n",
       "           7.1141e-01, 4.3092e-01],\n",
       "          [7.2435e-03, 3.8212e-01, 6.1842e-01,  ..., 7.1836e-01,\n",
       "           5.8678e-01, 3.5949e-01],\n",
       "          [4.9212e-03, 2.4092e-01, 3.7641e-01,  ..., 4.3617e-01,\n",
       "           3.6081e-01, 2.2904e-01]],\n",
       " \n",
       "         [[1.2979e-05, 4.7072e-04, 7.5895e-04,  ..., 8.5980e-04,\n",
       "           5.9369e-04, 3.6482e-04],\n",
       "          [4.7817e-04, 2.6503e-02, 4.4924e-02,  ..., 4.5125e-02,\n",
       "           3.3255e-02, 1.9291e-02],\n",
       "          [1.3278e-03, 4.8944e-02, 8.5003e-02,  ..., 7.4926e-02,\n",
       "           5.7590e-02, 3.2611e-02],\n",
       "          ...,\n",
       "          [1.2148e-03, 7.4416e-02, 1.4172e-01,  ..., 2.4620e-01,\n",
       "           1.0034e-01, 3.2151e-02],\n",
       "          [1.0146e-03, 6.2510e-02, 1.2260e-01,  ..., 8.2960e-02,\n",
       "           5.5662e-02, 2.9225e-02],\n",
       "          [6.3594e-04, 3.8338e-02, 1.2140e-01,  ..., 4.6406e-02,\n",
       "           3.2745e-02, 1.8216e-02]],\n",
       " \n",
       "         [[1.2470e-05, 4.4230e-04, 7.3237e-04,  ..., 8.6310e-03,\n",
       "           7.3520e-03, 4.9889e-03],\n",
       "          [4.4154e-04, 2.4307e-02, 4.2620e-02,  ..., 4.7020e-01,\n",
       "           3.9000e-01, 2.4560e-01],\n",
       "          [7.2642e-04, 4.2332e-02, 7.6475e-02,  ..., 7.8269e-01,\n",
       "           6.3802e-01, 3.8705e-01],\n",
       "          ...,\n",
       "          [6.7551e-04, 3.7756e-02, 6.2201e-02,  ..., 6.4526e-01,\n",
       "           6.0919e-01, 3.9134e-01],\n",
       "          [5.0953e-04, 2.7877e-02, 4.6050e-02,  ..., 4.8164e-01,\n",
       "           4.7408e-01, 3.1485e-01],\n",
       "          [3.2133e-04, 1.6397e-02, 2.6395e-02,  ..., 2.9073e-01,\n",
       "           2.8980e-01, 2.0055e-01]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1.3741e-04, 1.8248e-03, 1.2109e-03,  ..., 6.3023e-03,\n",
       "           5.1302e-03, 3.6033e-03],\n",
       "          [2.6065e-03, 7.6507e-02, 7.3557e-02,  ..., 3.1244e-01,\n",
       "           2.3693e-01, 1.4830e-01],\n",
       "          [3.5430e-03, 1.3209e-01, 1.3886e-01,  ..., 4.3387e-01,\n",
       "           2.7883e-01, 1.4321e-01],\n",
       "          ...,\n",
       "          [6.7575e-03, 3.4659e-01, 5.2861e-01,  ..., 8.3242e-01,\n",
       "           6.8350e-01, 4.1467e-01],\n",
       "          [6.0762e-03, 3.0671e-01, 4.7193e-01,  ..., 6.9167e-01,\n",
       "           5.6856e-01, 3.4985e-01],\n",
       "          [4.3555e-03, 2.0448e-01, 3.0625e-01,  ..., 4.2152e-01,\n",
       "           3.5153e-01, 2.2439e-01]],\n",
       " \n",
       "         [[1.1259e-05, 3.6281e-04, 5.6960e-04,  ..., 1.7234e-03,\n",
       "           1.0555e-03, 5.6637e-04],\n",
       "          [3.6337e-04, 1.9202e-02, 3.2091e-02,  ..., 9.8471e-02,\n",
       "           6.1700e-02, 3.1828e-02],\n",
       "          [5.7046e-04, 3.2400e-02, 5.7270e-02,  ..., 1.6145e-01,\n",
       "           1.0627e-01, 5.4646e-02],\n",
       "          ...,\n",
       "          [8.8201e-03, 4.8083e-01, 7.9510e-01,  ..., 2.2345e-01,\n",
       "           1.5560e-01, 8.2029e-02],\n",
       "          [7.4141e-03, 3.9320e-01, 6.3984e-01,  ..., 1.9798e-01,\n",
       "           1.3691e-01, 7.5933e-02],\n",
       "          [5.0030e-03, 2.4616e-01, 3.8616e-01,  ..., 1.6225e-01,\n",
       "           1.0113e-01, 7.8727e-02]],\n",
       " \n",
       "         [[1.2390e-05, 4.0633e-04, 4.6737e-04,  ..., 6.2014e-03,\n",
       "           5.2568e-03, 3.8315e-03],\n",
       "          [4.5682e-04, 2.4491e-02, 2.4043e-02,  ..., 3.0972e-01,\n",
       "           2.4495e-01, 1.6904e-01],\n",
       "          [7.8405e-04, 4.7289e-02, 9.6911e-02,  ..., 4.5268e-01,\n",
       "           2.8242e-01, 2.3702e-01],\n",
       "          ...,\n",
       "          [1.6683e-03, 1.1595e-01, 1.7153e-01,  ..., 2.7535e-01,\n",
       "           3.5163e-01, 2.6348e-01],\n",
       "          [1.3503e-03, 7.7127e-02, 1.2251e-01,  ..., 1.5219e-01,\n",
       "           2.4105e-01, 2.0911e-01],\n",
       "          [1.1101e-03, 6.1597e-02, 9.2050e-02,  ..., 8.2223e-02,\n",
       "           1.2655e-01, 1.3585e-01]]])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = (\n",
    "    data[\"x\"][0:n_train, :, :].unsqueeze(channel_dim).type(torch.float32).clone()\n",
    ")\n",
    "y_train = data[\"y\"][0:n_train, :, :].unsqueeze(channel_dim).clone()\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 50]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = test_resolutions.index(train_resolution)\n",
    "test_resolutions.pop(idx)\n",
    "n_test = n_tests.pop(idx)\n",
    "test_batch_size = test_batch_sizes.pop(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load(\n",
    "    Path(data_path).joinpath(f\"darcy_test_{train_resolution}.pt\").as_posix()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([[[False, False, False,  ..., False,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          ...,\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True]],\n",
       " \n",
       "         [[False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ...,  True,  True,  True],\n",
       "          [ True,  True,  True,  ...,  True,  True,  True],\n",
       "          [ True,  True,  True,  ...,  True,  True,  True]],\n",
       " \n",
       "         [[False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ...,  True,  True,  True],\n",
       "          [ True,  True,  True,  ...,  True,  True,  True],\n",
       "          [ True,  True,  True,  ...,  True,  True,  True]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ True,  True,  True,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          [False,  True,  True,  ..., False, False, False],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ...,  True,  True,  True],\n",
       "          [ True,  True,  True,  ...,  True,  True,  True],\n",
       "          [ True,  True,  True,  ...,  True,  True,  True]],\n",
       " \n",
       "         [[False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ...,  True,  True,  True],\n",
       "          [False,  True,  True,  ...,  True,  True,  True],\n",
       "          [False, False,  True,  ..., False,  True,  True]],\n",
       " \n",
       "         [[False,  True,  True,  ...,  True, False, False],\n",
       "          [False,  True,  True,  ..., False, False, False],\n",
       "          [False, False,  True,  ..., False, False, False],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False]]]),\n",
       " 'y': tensor([[[1.7661e-04, 4.8439e-03, 6.9495e-03,  ..., 2.6232e-03,\n",
       "           1.0451e-03, 5.3947e-04],\n",
       "          [4.8452e-03, 2.3539e-01, 3.6310e-01,  ..., 9.9066e-02,\n",
       "           5.8310e-02, 2.9887e-02],\n",
       "          [6.9597e-03, 3.6360e-01, 5.8077e-01,  ..., 1.3845e-01,\n",
       "           9.5713e-02, 5.0943e-02],\n",
       "          ...,\n",
       "          [6.1069e-03, 3.1437e-01, 5.1456e-01,  ..., 1.4254e-01,\n",
       "           1.0054e-01, 5.3703e-02],\n",
       "          [5.9591e-03, 3.0318e-01, 4.8525e-01,  ..., 1.0555e-01,\n",
       "           7.4734e-02, 4.0458e-02],\n",
       "          [4.3675e-03, 2.0668e-01, 3.1756e-01,  ..., 5.7983e-02,\n",
       "           4.1503e-02, 2.3121e-02]],\n",
       " \n",
       "         [[1.8190e-04, 5.1875e-03, 7.6306e-03,  ..., 7.8083e-03,\n",
       "           6.6993e-03, 4.6343e-03],\n",
       "          [5.1886e-03, 2.5772e-01, 4.0744e-01,  ..., 4.1439e-01,\n",
       "           3.4617e-01, 2.2186e-01],\n",
       "          [7.6390e-03, 4.0786e-01, 6.6902e-01,  ..., 6.5931e-01,\n",
       "           5.4304e-01, 3.3583e-01],\n",
       "          ...,\n",
       "          [7.7571e-04, 4.5469e-02, 8.2238e-02,  ..., 1.7266e-01,\n",
       "           1.2124e-01, 6.4153e-02],\n",
       "          [6.0504e-04, 3.4438e-02, 6.0652e-02,  ..., 1.2599e-01,\n",
       "           8.8440e-02, 4.7316e-02],\n",
       "          [3.7385e-04, 1.9904e-02, 3.3732e-02,  ..., 6.8632e-02,\n",
       "           4.8386e-02, 2.6522e-02]],\n",
       " \n",
       "         [[1.6488e-04, 4.0033e-03, 4.5842e-03,  ..., 4.9753e-03,\n",
       "           5.2279e-03, 4.0066e-03],\n",
       "          [4.1476e-03, 1.8634e-01, 2.3761e-01,  ..., 2.4281e-01,\n",
       "           2.5577e-01, 1.8326e-01],\n",
       "          [5.8565e-03, 2.8864e-01, 4.1150e-01,  ..., 3.7296e-01,\n",
       "           3.9123e-01, 2.7134e-01],\n",
       "          ...,\n",
       "          [1.0434e-03, 6.2944e-02, 1.1763e-01,  ..., 1.1415e-01,\n",
       "           8.3589e-02, 4.5963e-02],\n",
       "          [8.3105e-04, 4.9404e-02, 9.3520e-02,  ..., 8.2732e-02,\n",
       "           6.1455e-02, 3.4401e-02],\n",
       "          [5.6337e-04, 3.0069e-02, 5.3969e-02,  ..., 4.4701e-02,\n",
       "           3.4122e-02, 1.9801e-02]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1.1366e-05, 3.6746e-04, 5.8512e-04,  ..., 6.2074e-03,\n",
       "           5.8992e-03, 4.3092e-03],\n",
       "          [3.7675e-04, 1.9479e-02, 3.2767e-02,  ..., 3.1688e-01,\n",
       "           2.9765e-01, 2.0231e-01],\n",
       "          [1.4338e-03, 3.3772e-02, 5.5758e-02,  ..., 5.0006e-01,\n",
       "           4.6605e-01, 3.0598e-01],\n",
       "          ...,\n",
       "          [7.8736e-04, 4.6262e-02, 8.4102e-02,  ..., 1.3611e-01,\n",
       "           9.7426e-02, 5.2431e-02],\n",
       "          [6.2903e-04, 3.6202e-02, 6.5534e-02,  ..., 9.8339e-02,\n",
       "           7.1274e-02, 3.9072e-02],\n",
       "          [3.9374e-04, 2.1410e-02, 3.8303e-02,  ..., 5.3214e-02,\n",
       "           3.9263e-02, 2.2231e-02]],\n",
       " \n",
       "         [[1.7185e-04, 4.5085e-03, 6.1137e-03,  ..., 3.6186e-03,\n",
       "           4.6240e-03, 3.7237e-03],\n",
       "          [4.5603e-03, 2.1533e-01, 3.1341e-01,  ..., 1.8053e-01,\n",
       "           2.1847e-01, 1.6505e-01],\n",
       "          [6.5244e-03, 3.3290e-01, 5.0495e-01,  ..., 2.7543e-01,\n",
       "           3.1980e-01, 2.3548e-01],\n",
       "          ...,\n",
       "          [1.1304e-03, 6.4575e-02, 1.0865e-01,  ..., 2.4504e-01,\n",
       "           1.7018e-01, 8.8875e-02],\n",
       "          [1.8764e-03, 6.0648e-02, 8.2372e-02,  ..., 2.0809e-01,\n",
       "           1.3042e-01, 6.6187e-02],\n",
       "          [2.1342e-03, 5.2109e-02, 4.8604e-02,  ..., 1.6015e-01,\n",
       "           7.0970e-02, 3.6746e-02]],\n",
       " \n",
       "         [[1.3091e-04, 1.0402e-03, 1.1658e-03,  ..., 5.2448e-04,\n",
       "           3.3449e-03, 3.1064e-03],\n",
       "          [2.3931e-03, 5.6136e-02, 7.0957e-02,  ..., 7.7284e-02,\n",
       "           1.5082e-01, 1.2273e-01],\n",
       "          [3.5553e-03, 1.3470e-01, 1.3737e-01,  ..., 2.2303e-01,\n",
       "           1.8601e-01, 1.2410e-01],\n",
       "          ...,\n",
       "          [1.2651e-03, 7.6803e-02, 1.4175e-01,  ..., 8.5182e-01,\n",
       "           7.1614e-01, 4.3675e-01],\n",
       "          [9.1064e-04, 5.4038e-02, 9.8101e-02,  ..., 6.6030e-01,\n",
       "           5.6875e-01, 3.5541e-01],\n",
       "          [5.2004e-04, 2.9288e-02, 5.1685e-02,  ..., 3.8861e-01,\n",
       "           3.4368e-01, 2.2395e-01]]])}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = data[\"x\"][:n_test, :, :].unsqueeze(channel_dim).type(torch.float32).clone()\n",
    "y_test = data[\"y\"][:n_test, :, :].unsqueeze(channel_dim).clone()\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "if encode_input:\n",
    "    if encoding == \"channel-wise\":\n",
    "        reduce_dims = list(range(x_train.ndim))\n",
    "    elif encoding == \"pixel-wise\":\n",
    "        reduce_dims = [0]\n",
    "\n",
    "    input_encoder = UnitGaussianNormalizer(dim=reduce_dims)\n",
    "    input_encoder.fit(x_train)\n",
    "    #x_train = input_encoder.transform(x_train)\n",
    "    #x_test = input_encoder.transform(x_test.contiguous())\n",
    "else:\n",
    "    input_encoder = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if encode_output:\n",
    "    if encoding == \"channel-wise\":\n",
    "        reduce_dims = list(range(y_train.ndim))\n",
    "    elif encoding == \"pixel-wise\":\n",
    "        reduce_dims = [0]\n",
    "\n",
    "    output_encoder = UnitGaussianNormalizer(dim=reduce_dims)\n",
    "    output_encoder.fit(y_train)\n",
    "    #y_train = output_encoder.transform(y_train)\n",
    "else:\n",
    "    output_encoder = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db = TensorDataset(\n",
    "    x_train.to(device),\n",
    "    y_train.to(device),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_db,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_db = TensorDataset(\n",
    "    x_test.to(device),\n",
    "    y_test.to(device),\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_db,\n",
    "    batch_size=test_batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test db at resolution 32 with 50 samples and batch-size=32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_loaders = {train_resolution: test_loader}\n",
    "for (res, n_test, test_batch_size) in zip(\n",
    "    test_resolutions, n_tests, test_batch_sizes\n",
    "):\n",
    "    print(\n",
    "        f\"Loading test db at resolution {res} with {n_test} samples \"\n",
    "        f\"and batch-size={test_batch_size}\"\n",
    "    )\n",
    "    data = torch.load(Path(data_path).joinpath(f\"darcy_test_{res}.pt\").as_posix())\n",
    "    x_test = (\n",
    "        data[\"x\"][:n_test, :, :].unsqueeze(channel_dim).type(torch.float32).clone()\n",
    "    )\n",
    "    y_test = data[\"y\"][:n_test, :, :].unsqueeze(channel_dim).clone()\n",
    "    del data\n",
    "    #if input_encoder is not None:\n",
    "        #x_test = input_encoder.transform(x_test)\n",
    "\n",
    "    test_db = TensorDataset(\n",
    "        x_test,\n",
    "        y_test,\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_db,\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "    test_loaders[res] = test_loader \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if positional_encoding:\n",
    "    pos_encoding = PositionalEmbedding2D(grid_boundaries=grid_boundaries)\n",
    "else:\n",
    "    pos_encoding = None\n",
    "data_processor = DefaultDataProcessor(\n",
    "    in_normalizer=input_encoder,\n",
    "    out_normalizer=output_encoder,\n",
    "    positional_encoding=pos_encoding\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x70e9ae7ab110>,\n",
       " {16: <torch.utils.data.dataloader.DataLoader at 0x70e9c5931cd0>,\n",
       "  32: <torch.utils.data.dataloader.DataLoader at 0x70ea2455add0>},\n",
       " DefaultDataProcessor(\n",
       "   (out_normalizer): UnitGaussianNormalizer()\n",
       " ))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader, test_loaders, data_processor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralop_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
